\documentclass[12pt, a4paper]{article}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
colorlinks=false,% hyperlinks will be black
  linkbordercolor=blue,% hyperlink borders will be red
  pdfborderstyle={/S/U/W 1}% border style will be underline of width 1pt
}

\usepackage{listings}
\usepackage{tikz-network}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\title{Algortihms and datastructures}
\date{2022}
\author{Kristoffer Klokker}
\begin{document}
	\maketitle
	\clearpage
	\tableofcontents
	\clearpage
		\section{Algorithm analyse}
			An algorithm must stop for all input and give the correct output.\\
			An algorithms quality is determined from:
			\begin{itemize}
				\item Speed
				\item Memory used
				\item Complexity of implmentation
				\item Other use cases like stability
			\end{itemize}
			\subsection{Algorithm speed}
				The measure speed and memory the worst case of the algorithm is always used due to its simplicity in calculations and gurantee. \\
				The measurement is used as a function of input size using the big O notation, which says for a given input with $n$ size it will take $n^2$  time to run for example.\\
				In reality the speed of an algorithm is not equivalent to the input size due to small constant in every operation but these are so miniscule, they should not be considered. A function may be tuned to have lower constants but this is more a topic for theoretical algorithm optimization.\\
				When comparing algorithm speed the math relations symbols are replaced as such:
				\begin{itemize}
					\item $= -\; \Theta$ Theta
					\item $\leq -\; O$ O
					\item $\geq - \;\Omega$ Omega
					\item $< -\; o$ little o
					\item $> - \;\omega$ little omega
				\end{itemize}
				When comparing the speed of two algorithms the following methods can be used:
				\begin{align*}
					\lim\limits_{n\rightarrow \infty}\frac{f(n)}{g(n)}>&0  &&f(n)=\Theta(g(n))\\
					\lim\limits_{n\rightarrow \infty}\frac{f(n)}{g(n)}=&0  &&f(n)=o(g(n))
				\end{align*}
				The generel speeds of alogirthms in order is:
				$$1,\; \log_2n,\; \sqrt{n},\; n,\; n\log_2n,\; n\sqrt{n},\; n^2,\; n^3,\; n^{10},\; 2^n$$
			\subsection{Ram-model}
				To analyze an algorithm a model is used most often the ram model.
				The ram model is a very simple interpretation of a computer.\\
				The model consist of a CPU, memory and basic operation (add, sub, mult, shift, move, jump)\\
				The time of the algorithm is then measured in amount of basic operations done.\\
				The memory is determined as the amount of memory cell used.
		\section{Sorting algorithms}
			Sorting algorithms are used to sort an array of items in an ascending order.\\
			\subsection{Insertion sort}
				One of the most simple sorting algorithms.\\
				Works by going through the list from index 1 and moves moves every entry before the element 1 up until the element is to the right of an element smaller than the element.\\
				This will therefore have a run time of $O(n^2)$ due to the scenario where the array is in decending order where it will moves $n-1,n-2,..,1$ elements which is $\sum\limits_{j=1}^nj=\frac{(n-1)n}{2}\leq\frac{n^2-n}{2}=n^2$
				\begin{algorithmic}[1]
					\State Insertion-Sort($A,n$)
					\For{$j=1$ to $n$}
						\State $key = A[j]$
						\State $i=j-1$
						\While {$i\geq0$ and $A[i] > key$}
							\State $a[i+1]=A[i]$
							\State $i = i-1$
						\EndWhile
						\State $A[i+1]=key$
					\EndFor
				\end{algorithmic}
			\subsection{Selectionsort}
				Selectionsort is done by taking all elements and then searching for the smallest element and inserting it into the list.\\
				This will result in the same run time $O(n^2)$ due to the same reasoning as insertionsort.\\
				\begin{algorithmic}[1]
					\State Selection-Sort($A,n$)
					\State Array output $= new Array(n)$
					\State $i = 0$
					\While {$i<n$}
						\State $output[i] = A[i]$
						\State $A[i].remove()$
						\State $i++$
					\EndWhile
				\end{algorithmic}
			\subsection{Merge sort}
				Merge sort is an sorting algorithm which is based on the simple merge of two sorted list. Here a new list is created and for every element of the two list, is the lowest value choosen from the first place of the list and putted into the new list. This is is done until both list are empty and when sorting the undefined element is simply equal to infinity when compared.\\[5mm]
				To make this into an sorting algorithm, every element in an unsorted list is made into to list of length 1. Then merge is performed on the lists in pairs of two, from which new sorted lists emerges. This is done until a single lists is left with all the elements which is now sorted. \\[4mm]
				 The speed of this sorting algorithm will therefore be $n\lceil\log_2n\rceil$ due to for every level of merge there will be performed $n$ operations. The amount of levels of merges are equal to $\log_2n$ due to first there will be $n$ lists, then $n/2$ lists, and then $n/2^2$. This will continue until $n/2^k=1$ which rearranged to find $k$ will be $log_2n=k$.\\
				 The ceiling of $\log_2n$ is due to in scenarios where the list length is not a potens of two and a list has no pair in this case it will go on to the next level and thus creating a new level once the list has 1 more item than a potens of two.
				 \begin{figure}
				 	\centering
				 	\includegraphics[width=300px]{assets/mergeSort.png}
				 	\caption{Merge sort list split up illustration from j2eereference.com}
				\end{figure}
			\subsection{Quick sort}
				A recursive sorting algorithm which runs mostly at $O(n\log n)$ time but worst case is $O(n^2)$.\\
				The sorting algorithm is smart due to it not needing subarrays copies and working in the original array.\\
				The main idea, is choosing an element in the array, then sorting everything in two baskets lower or equal and higher.\\
				Then the element is placed between the two baskets and quick sort is ran upon the two baskets.
				\begin{algorithmic}[1]
					\State Quick-Sort($A,p,r$)
					\If {$r \leq p$} \State return; \EndIf
					\State $i = p$
					\For {$j=p$ to $r$}
						\If {$A[j] \leq x$}
							\State $temp = A[i]$;
							\State $A[i] = A[j]$;
							\State $A[j] = temp$;
							\State $i=i+1$;
						\EndIf
					\EndFor
					\State $temp = A[i]$;
					\State $A[i] = A[r]$;
					\State $A[r] = temp$;
					\State $Quick-Sort(A,p,i-1)$;
					\State $Quick-Sort(A,i+1,r)$;
				\end{algorithmic}
				The arguments is the array $A$ the start of the sort $p$ and the end $r$.\\
				Here it is seen that in the for loop if the current element is smaller than the choosen element it is switched with the first element in larger basket such that the smaller basket is in front.\\
				In case the element is larger nothing is done. Then at the end the choosen element is switched with the first element of the larger bucket.
			\subsection{Heap sort}
			\label{sec:HeapSort}
				Heap sort is based on the Heap and runs in $O(n\log n)$ and uses no extra space. The heap which is a binary tree which follows heap order. The heap order dictates that a parent should always be greater or equal to its children. \\
				The tree should also have heap shape, which is all branches should be the saem height with a maximum difference of 1. In case of a larger branch it should be on the left side of the tree.\\
				This heap can is illustrated as a tree but is actually an array. Here the arrays first entry is the root, the roots children will then be at index 2 times index and 2 times index + 1\\
				This means the parent is at $\lfloor i/2 \rfloor$ relative to the child.
				\begin{figure}[h!]
					\includegraphics[width=300px]{assets/heapAsArray.png}
					\center
					\caption{The heap illustration and the heap array}
				\end{figure}
				Heap sort then uses the heap array, and when sorting the first entry or the root is switched with the last entry of the heap array.\\
				By this the size of the heap array shrinks with 1 and the rest of the array is the sorted array.\\
				When switched the heap is reconstructed.\\
				The reconstruction process is as follows:
				\label{code:MaxHeapify}
				\begin{algorithmic}[1]
					\State Max-Heapify($A,i,n$)
					\State $l = 2i$;
					\State $r =2i+1$;
					\If {$l\leq n$ and $A[l] > A[i]$}
						\State $largest = l$;
					\Else
						\State $largest = i$;
					\EndIf
					\If {$r \leq n$ and $A[r] > A[largest]$}
						\State $largest = r$;
					\EndIf
					\If {$largest != i$}
						\State $temp = A[i]$;
						\State $A[i] = A[largest]$;
						\State $A[largest] = temp$;
						\State $Max-Heapify(A,largest,n)$;
					\EndIf
				\end{algorithmic}
				The $n$ is used as the length of the heap and $i$ is the current node to check.\\
				The first if check if the left child is larger, the second checks if the right child is larger, and the third check if something should be changed and if then call itself upon the newly changed child.\\
				With this function in mind heap sort becomes: 
				\begin{algorithmic}[1]
					\State HeapSort($A,n$)
						\State Max-Heapify(A,n);
						\For {$i=n$; $2 < i$; $i--$;}
							\State $temp = A[i]$;
							\State $A[i] = A[0]$;
							\State $A[0] = temp$;
							\State Max-Heapify($A,1,i-1$);
						\EndFor
				\end{algorithmic}
				The run time of this is $O(n\log n)$ dut to the trees height will maximally be $\log n$ which will be the number of exhanges $n$ times.
			\subsection{Counter sort}
				Counter sort works in $O(n+k)$ time on arrays of objects represented by integers by having an array with $k$ length. Where $k$ is the highest integer.\\
				It then goes through the orginal array and takes every elements value and uses it as index for the $k$ array and itterate by 1.\\
				Then a new array with length $n$ is made and every element in the $k$ array is added to the new array with the value of elements index as many times as the value of element.
			\subsection{Radix sort}
				Radis sort runs in $O(d(n+k))$ time, where $d$ is amount of digits in largest number and $k$ is the max value of the base of the number system. First Counter sort is done upon every elements lasts digit. Then from the new array counter sort is done again on the second lasts digit. This is repeated as many times as the most digits in the largest number.\\
				This therefore means in the counter sort, the $k$ array only has the length the max value of the number system and instead of itterate it is now a list with references.\\
				This therefore solves the issue of now knowing the largest digit, and the temperary array is a lot shorter.
		\section{algorithms}
			\subsection{Divide-and-Conquer algorithms}
				The same as recursive algorithms.\\
				When illustrated a tree is often used to describe the calls and calls backs
			\subsection{Lower bound of comparison sorting algorithms}
				To find the lower bound of comparison sorting algorithms, the comparison model can be used.\\
				The model consist of a binary tree where every branch is a comparison, and the leafs are possible answers.\\
				The run time here is the comparisons, therefore the run time to a leaf is the length of the path. Therefore making worst case height of tree.\\
				It can therefore be seen when sorting an array of $n$ elements there are $n!$  different rearrangement.\\
				Each rearrangement needs a leaf therefore the tree has to have $n!$ leafs.\\
				The maximum amount of leafs in a tree is $2^h$ where $h$ is the height.\\
				Therefore the minimum amount of height will be $2^h\geq n! \rightarrow h\geq \log n!$\\
				\begin{align*}
					h&\geq \log n!\\
					&=\log (n\cdot (n-1)\cdot (n-2)\cdot ...)\\
					&=\log n + \log(n-1) + ...\\
					&\geq \frac{n}{2}\cdot \log(\frac{n}{2})\\
					&=\frac{n}{2}(\log(n)-1)
				\end{align*}		
				It can here be derived via the summation, that $h=\Omega(n\log n)$
			\subsection{Maximum summation of sequential elements in array}
				The most simple version of this, is two pointers with start of seqence and end, where for each element in the array the start is appointed from which all possible ends if summed and the largest value is taken.\\
				This means the algorithm will take $O(n^3)$ due to first every element being appointed as start, from which all elements is tested as the end of sequence, from which the summation is done. This will round up to the worst case being $O(n^3)$\\
				This can be optimized to a run time of $O(n)$, by instead of recalculating everything for every pointer a pointer is started at the start from which an end pointer is also at the start.\\
				Here the largest summation is found, if it is larger than the best sequence it is assigned. Then the end pointer is moved forward and the new element is summed to the recent best sequence. If the sequence summation is now below zero the start pointer is moved to the end otherwise the end pointer is moved forward.
				\begin{algorithmic}[1]
					\State MaxSequence($A$)
					\State $max = 0$
					\State $tempMax = 0$
					\State $i = 0$
					\While {$i<A.length$}
						\State $tempMax = max(tempMax + A[i], 0)$
						\State $max = max(max, tempMax)$
						\State $i++$
					\EndWhile
				\end{algorithmic}
			\subsection{Hashing}
				Hashing is method of getting a idenfiable key to some data.\\
				While the data may have range of indexible data, the hash function is a way to get a smaller index.\\
				The motivation is seen in the following example.\\
				Storing 5 credit card numbers in an array with size of every possible creditcard number would not be optimal.\\
				A simple solution is taking the integer value of the object an using modulo on the desired size of the array.\\
				This will create problems due to some object will collide, as a resolution a list can be created at every index which then can be added.\\
				This will result in slower run time worst case being $O(1)$ for the index and $O(n)$ for finding it in the list.\\
				An alternative is a binary search tree to optimize it to $O(\log n)$.\\
				The ideal hash function will spread out objects as much as possible and a simple an reliable function being
				$$h(k)=((a\cdot k+b) \;mod\; p)\; mod\; m$$
				Where p is a prime larger than the size of the array and a and b are random constant integers $1\leq a \leq p-1$ and $0\leq b \leq p-1$
				\subsubsection{Open addressing}
					An alternative to having a list as hash chain, is using different methods of finding next open position.\\
					The following methods uses a given hash function $h(k,i)$, where $k$ is the given value, and $i$ is the shift.\\
					For every same entry the $i$ variable is increased by 1.\\
					\textbf{Linear hashing}: if the index is taken $i$ is added to index\\
					\textbf{Quadratic hashing}: if the index is taken $i^2$ is added to index\\
					\textbf{Double hashing}: If the index is taken $i\cdot h(k)$ is added.\\
					The reason to use methods which space out index more is to prevent clustering, which can worsen search times.
		\section{Datastructures}
			\subsection{Priority queues}
				Priority queues are based on the heap structure, which is described in \hyperref[sec:HeapSort]{Heap sort}.\\
				Here the queue objects are given a key from which the value iindicates the priority of the object.\\
				It can here selected if the wanted heap is a max heap or min heap, but most often a max heap.\\
				For the queue to work it uses the following functions:
				\begin{itemize}
					\item Max-Heapify(A,i,n) - makes the array in max heap order with the length $n$ and looks at node $i$
					\item Heap-Extract-Max(A,n) - returns the highest priority object and remove from heap
					\item Heap-Increase-Key(A,i,key) - modifies the key $i$ to a higher value $key$ and place it correctly in heap
					\item Max-Heap-Insert(A,key) - Inserts $key$ into the heap
				\end{itemize}
				\subsubsection{Pseudo code}
					\hyperref[code:MaxHeapify]{Max-Heapify code}
				\begin{algorithmic}[1]
					\State Heap-Extract-Max($A,n$)
					\If {$n < 1$}
						\State error 'Heap underflow'
					\EndIf
					\State $max = A[0]$
					\State $A[0] = A[n-1]$
					\State $Max-Heapify(A,1,n-1)$
					\State return $max$
				\end{algorithmic}
				.\\[3mm]
				\begin{algorithmic}[1]
					\State Heap-Increase-Key($A,i,key$)
					\If {$key < key[i]$}
						\State error 'new key is smaller than current'
					\EndIf
					\While {$i > 0$ AND $A[Parent(i)] < A[i]$}
							\State $temp = A[i]$;
							\State $A[i] = A[i/2]]$;
							\State $A[i/2] = temp$;
							\State $i=i/2$
					\EndWhile
					\end{algorithmic}
				.\\[3mm]
					\begin{algorithmic}[1]
						\State Max-Heap-Insert(A,key,n)
						\State $A.insert(n,-\infty)$
						\State $Heap-Increase-Key(A,n,key)$
					\end{algorithmic}
			\subsection{Binary search tree}
				A binary search tree is a tree which follows inorder.\\
				Inorder states that for a node the left child is smaller and the right child is larger.\\
				By this to get the ordered list a recursive call can be:
				\begin{algorithmic}[1]
					\State list-tree(x)
					\State list-tree(x.left)
					\State print(x)
					\State list-tree(x.right)
				\end{algorithmic}
				This will go through the tree in order, this pseudo code will here print every key, and it should be noded the code does not handle NIL objects.\\
				This operation takes $O(n)$ run time and all other oeparations as mention below will run in $O(height)$. The balanced binary tree will therefore have a height of $\log_2 (n+1) \leq $ height and making the run time $O(\log n)$.
				\subsubsection{Searching a binary tree}
					To search the tree, a comparison is done at each node from which the right child is choosen:
					\begin{algorithmic}[1]
						\State Tree-Search(x,k)
						\If {$x == NIL$ OR $k==key[x]$}
							\State return $x$
						\EndIf
						\If {$k < x.key$}
							\State return $Tree-Search(x.left,k)$
						\Else
							\State return $Tree.search(x.right,k)$
						\EndIf
					\end{algorithmic}
					Find the lowest key, .					
					\begin{algorithmic}[1]
						\State Tree-Minimum(x)
						\While{$x.left != NIL$}
							\State $x = x.left$
						\EndWhile
						\State return $x$
					\end{algorithmic}
					To find the smallest key bigger than key $x$. Works by if right side is not NIL it will be the minumum on the right side otherwise it moves up to the parent until the given node is the left child to the parent.
					\begin{algorithmic}[1]
						\State Tree-Successor($x$)
						\If {$x.right != NIL$}
							\State return Tree-Minimum($x.right$)
						\EndIf
						\State $y=x.parent$
						\While{$y!=NIL$ and $x==y.right$}
							\State x= y
							\State y = y.parent
						\EndWhile
						\State return $y$
					\end{algorithmic}
				\subsubsection{Inserting in the tree}
				\label{sec:BSTInsert}
					\begin{minipage}{0.49\textwidth}
						Tree-Insert(T,z)
						\begin{algorithmic}[1]
							\State $y = NIL$
							\State $x = T.root$
							\While {$x $!=$ NIL$}
								\State $y = x$
								\If {$z.key < x.key$}
									\State $x = x.left$
								\Else
									\State $x = x.right$
								\EndIf
							\EndWhile
							\State $z.p = y$
							\If {$y == NIL$}
								\State $T.root = z$
							\ElsIf {$z.key < y.key$}
								\State $y.left = z$
							\Else
								\State $y.right = z$
							\EndIf								
						\end{algorithmic}
					\end{minipage}
					\begin{minipage}{0.49\textwidth}
						$y$ is the current nodes parent and $x$ is the current node.\\
						Then in the while loop the node is moved down in the correct direction, until it hits NILL.\\
						Then a check is done, if $y$ is NILL the tree is empty and we are at root.\\
						Otherwise the node is placed according to the correct direction to parent.
					\end{minipage}
				\clearpage
				\subsubsection{Deleting a node in the tree}
				When removing an element there is three case according to the number of children equal to NIL:
						\begin{itemize}
							\item 2 - The element is replaced by NIL
							\item 1 - The parent reference is changed to the child
							\item 0 - The succesor to the remove node is inserted instead.
						\end{itemize}
					\begin{minipage}{0.49\textwidth}
						Tree-Delete(T,z)
						\begin{algorithmic}[1]
							\If {$z.left == NIL$}
								\State $Transplant(T,z,z.right)$
							\ElsIf {$z.right == NIL$}
								\State $Transplant(T,z,z.left)$
							\Else
								\State $y=Tree-Min(z.right)$
								\If {$y.p != z$}
									\State $Transplant(T,y,y.right)$
									\State $y.right = z.right$
									\State $y.right.p = y$
								\EndIf
								\State $Transplant(T,z,y)$
								\State $y.left = z.left$
								\State $y.left.p = y$
							\EndIf
						\end{algorithmic}
					\end{minipage}
					\begin{minipage}{0.49\textwidth}
						The delete method uses a $Transplant$ function which arguments are:\\
						 Tree, node1, node2. \\
						Which then switches node1 and node2.\\
						The first if and else if covers the first case and second case with 2 or 1 child equal to NILL\\
						In this case the other child is swapped with the node.\\[4mm]
						The else covers the last case of 0 NILL children.\\
						First the succesor is found by the minimum node of $z$'s right child.\\
						The the if at line 7, checks for if the MIN node is not $z$'s right child.\\
						 If not then the min node $y$ is rotated up as the root of $z$'s right branch, with its parent as right child and the parent left child being $y$s right child.\\
						 Then $y$ substitutes $z$ and takes $z$ left child aswell.\\
						 
					\end{minipage}
				
					
			\subsection{Balanced binary tree}
				The idea behind a balanced binary tree, is a node has another bit representing a color red or black.\\
				The rule is then:
				\begin{itemize}
					\item The root has to be black
					\item A red notes child can not be red
					\item Leafs has to be black
				\end{itemize}
				The rules ensures that the roots childrens height will atmost be different by a factor by 2.\\
				This is due to one branch being all black and the other being red and black all the way.\\
				The implementation of a balanced tree will not be slower due to all the manipulations still running at constant time $O(1)$.
				\subsubsection{Rotation of node}
					Rotations are a needed operation on a node in order to insert and remove nodes, and still uphold balance.\\
					\begin{figure}[h!]
						\center
						\includegraphics[width=300px]{assets/rotation.png}
						\caption{Illustration of rotation}
					\end{figure}
					
					\begin{minipage}{0.49\textwidth}
						Left-Rotate(T,x)
						\begin{algorithmic}[1]
							\State y = x.right
							\State x.right = y.left
							\If {y.left != T.nil}
								\State y.left.p =x
							\EndIf
							\State y.p = x.p
							\If {x.p == T.nil}
								\State T.root =y
							\ElsIf {x ==x.p.left}
								\State x.p.left = y
							\Else 
								\State x.p.right = y
							\EndIf
							\State y.left = x
							\State x.p = y
						\end{algorithmic}
					\end{minipage}
					\begin{minipage}{0.49\textwidth}
						First y is established and $\beta$ is moved at line 2.\\
						The if at line 3 changes the y left parent reference to x if not nil\\
						The if at line 7 set y as root if x is root\\
						The if at 9 and 11 sets x parents reference to x to y according to direction.
						It can also be seen the rotation will keep inorder due to:
						$$\alpha \leq x \leq \beta \leq  y \leq \gamma$$
						Which is followed both before and after rotation.
					\end{minipage}
			\subsubsection{Insertion in tree}
				First the new element is inserted per usual \hyperref[sec:BSTInsert]{inserting in a binary search tree} as a red node.
				After this rebalancing may be needed, from which four sceanrios are possible:
				\begin{enumerate}
					\item z = root
					\item z.uncle = red
					\item z.uncle = black (trangle)
					\item z.uncle = black (line)
				\end{enumerate}
				\begin{figure}[h!]
					\center
					\includegraphics[width=300px]{assets/blackRedTreeTriangeLine.png}
					\caption{A triange shape on left and line shape on right.}
				\end{figure}
				1.\\
				The nodes color should be switched to black\\
				2.\\
				Parent, uncle and grandparent change color\\
				3.\\
				Rotate z.parent in opposite direction of z\\
				4.\\
				Recolor parent and grandparent an rotate z.granparent in oppsite direction of z\\
				RB-Insert-Fix(T,z)
				\begin{algorithmic}[1]
					\While {$z.p.color == red$}
						\If {$z.p == z.p.p.left$}
							\State $y = z.p.p.right$\
							\If {$y.color == red$}
								\State $z.p.color = BLACK$
								\State $y.color = BLACK$
								\State $z.p.p.color = RED$
								\State $z = z.p.p$
							\Else
								\If {$z == z.p.right$}
									\State $z = z.p$
									\State $Left-Rotate(T,z)$
								\EndIf
								\State $z.p.color = BLACK$
								\State $z.p.p.color = RED$
								\State $Right-Rotate(T,z.p.p)$
							\EndIf
						\Else
							\State line 3 - 15 but with left instead of right
						\EndIf
					\EndWhile	
					\State $T.root.color = BLACK$
				\end{algorithmic}
				The while loop runs until, the parent node is not red.\\
				The first check is if the current branch is to the grandparents left.\\
				Then the uncle $y$ is defined.\\
				Then the uncles color is checked. If red then color changes according to case 2.\\
				If black, then if the triangle shape in case 3 is present the rotation is done. From which case 4 will accour and recolor and rotation is done.
			\subsubsection{deletetion in tree}
				First a deletion is done, as normal. The only difference is a fixup function is called if the removed node is black or the successor node was black.\\
				This due to a black therefore being removed or recolored and therefore the black length is not equal on both sides of root.\\
				The fixup function has 4 cases which has to be fixed.
				\begin{enumerate}
					\item red sibling
					\item black sibling with 2 black children
					\item black sibling and nearest child is red and furthest is black
					\item black sibling and the furthest child is red
				\end{enumerate}
				The special child is x and its sibling is w\\
				1. the parent and $w$ changes color and left rotation on parent\\
				2. $w$ changes color to red, $x$ pointer is parent and parent is red\\
				3. $w$ and its red child $\gamma$ changes color and right rotation is performed on $\gamma$\\
				4. $w$ changes color to $x$'s parent, which changes color to black aswell $w$ right child. Left rotation on $x$'s parent\\
				RB-Delete-Fix(T,x)
				\begin{algorithmic}[1]
					\While {$x != T.root$ and $x.color == BLACK$}
						\If {$x== x.p.left$}
							\State $w = x.p.right$
							\If {$w.color == RED$}
								\State $w.color = BLACK$
								\State $x.p.color = RED$
								\State $Left-Rotate(T,x.p)$
								\State $w = x.p.right $
							\EndIf
							\If {$w.left.color == BLACK$ and $w.right.color == BLACK$}
								\State $w.color = RED$
								\State $x = x.p$
							\Else
								\If {$w.right.color == BLACK$}
									\State $w.left.color = BLACK$
									\State $w.color = RED$
									\State $Right-Rotate(T,w)$
									\State $w = x.p.right$
								\EndIf
								\State $w.color = x.p.color$
								\State $x.p.color = BLACK$
								\State $w.right.color = BLACK$
								\State $Left-Rotate(T,x.p)$
								\State $x = T.root$
							\EndIf
						\Else
							\State line 3 - 25 with right and left exhanged
						\EndIf
					\EndWhile
					\State $x.color = BLACK$
				\end{algorithmic}
				
				The code consist of a while loop which only terminates when x is root or x is black.\\
				This is due to if x is the root the color can be changed to black and it will have the same black length on both branches.\\
				The while loop states by checking if the current working branch is to the parents left.\\
				Then the first if covers case 1, and the sibling pointer moves up to $x$'s uncle.\\
				The second if covers case 2, notice here it is guranteed due to the first case that the sibling is black.\\
				The else covers case 3 and 4, where 3 is in the inner if. \\
				It can here be seen that 1 can lead up to all cases, 2 does not lead up to 3 or 4 and 3 always leads up to 4.
				
					
				
					
					
						
				
			
				
	
				
\end{document}
